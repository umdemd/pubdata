Name,Title,Author,Venue,Year,PaperLink,ThumbnailLink,VideoLink,Abstract,Award
DexteriSync,DexteriSync: A Hand Thermal I/O Exoskeleton for Morphing Finger Dexterity Experience,"Ximing Shen, Youichi Kamiyama, Kouta Minamizawa, and Jun Nishida",ACM UIST 2024,2024,https://doi.org/10.1145/3654777.3676422,https://images.squarespace-cdn.com/content/66214dcc26841d4d27c5ba9b/1722327087111-JGOQZGZKC82Y1405GC3V/DX.jpg?format=500w,,"Skin temperature is an important physiological factor for human hand dexterity. Leveraging this feature, we engineered an exoskeleton, called \textit{DexteriSync}, that can dynamically adjust the user's finger dexterity and induce different thermal perceptions by modulating finger skin temperature. This exoskeleton comprises flexible silicone-copper tube segments, 3D-printed finger sockets, a 3D-printed palm base, a pump system, and a water temperature control with a storage unit. By realising an embodied experience of compromised dexterity, DexteriSync can help product designers understand the lived experience of compromised hand dexterity, such as that of the elderly and/or neurodivergent users, when designing daily necessities for them. We validated DexteriSync via a technical evaluation and two user studies, demonstrating that it can change skin temperature, dexterity, and thermal perception. An exploratory session with design students and an autistic compromised dexterity individual, demonstrated the exoskeleton provided a more realistic experience compared to video education, and allowed them to gain higher confidence in their designs. The results advocated for the efficacy of experiencing embodied compromised finger dexterity, which can promote an understanding of the related physical challenges and lead to a more persuasive design for assistive tools.",
JetUnit,JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets,"Jiasheng Li, Zeyu Yan, Jun Nishida, and Huaishu Peng",ACM UIST 2024,2024,https://doi.org/10.1145/3654777.3676440,https://images.squarespace-cdn.com/content/66214dcc26841d4d27c5ba9b/1722750577622-6G13S8JCHVVLWPND8RMC/JU.jpeg?format=500w,https://youtu.be/ES31QBIXrLQ?si=wj8zqurmNz-3jMQb,"We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets. The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry. In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study. We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences. The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality.",
Morphing Identity,Morphing Identity: Exploring Self-Other Identity Continuum through Interpersonal Facial Morphing Experience,"Kye Shimizu, Santa Naruse, Jun Nishida, and Shunichi Kasahara",ACM CHI 2023,2023,https://dl.acm.org/doi/10.1145/3544548.3580853,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719540559817-ZMM5XR7OY7A4WW7CZGZ0/MI.jpg?format=500w,https://youtu.be/rfmIuaV9Ny0?si=N1lODSuLcm_OhJ2V,"We explored continuous changes in self-other identity by designing an interpersonal facial morphing experience where the facial images of two users are blended and then swapped over time. Both users’ facial images are displayed side by side, with each user controlling their own morphing facial images, allowing us to create and investigate a multifaceted interpersonal experience. To explore this with diverse social relationships, we conducted qualitative and quantitative investigations through public exhibitions. We found that there is a window of self-identification as well as a variety of interpersonal experiences in the facial morphing process. From these insights, we synthesized a Self-Other Continuum represented by a sense of agency and facial identity. This continuum has implications in terms of the social and subjective aspects of interpersonal communication, which enables further scenario design and could complement findings from research on interactive devices for remote communication.",
DigituSync,DigituSync: A Dual-User Passive Exoskeleton Glove That Adaptively Shares Hand Gestures,"Jun Nishida, Yudai Tanaka, Romain Nith and Pedro Lopes",ACM UIST 2022,2022,https://dl.acm.org/doi/10.1145/3526113.3545630,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719298952752-TFARM3411YIPMKMKXOPX/DS.JPG?format=500w,https://youtu.be/XpOKhXOXnt8,"We engineered DigituSync, a passive-exoskeleton that physically links two hands together, enabling two users to adaptively transmit finger movements in real-time. It uses multiple four-bar linkages to transfer both motion and force, while still preserving congruent haptic feedback. Moreover, we implemented a variable-length linkage that allows adjusting the force transmission ratio between the two users and regulates the amount of intervention, which enables users to customize their learning experience. DigituSync's benefits emerge from its passive design: unlike existing haptic devices (motor-based exoskeletons or electrical muscle stimulation), DigituSync has virtually no latency and does not require batteries/electronics to transmit or adjust movements, making it useful and safe to deploy in many settings, such as between students and teachers in a classroom. We validated DigituSync by means of technical evaluations and a user study, demonstrating that it instantly transfers finger motions and forces with the ability of adaptive force transmission, which allowed participants to feel more control over their own movements and to feel the teacher's intervention was more responsive. We also conducted two exploratory sessions with a music teacher and deaf-blind users, which allowed us to gather experiential insights from the teacher's side and explore DigituSync in applications.",
Whose touch is this?,Whose touch is this?: Understanding the Agency Trade-off Between User-driven touch vs. Computer-driven Touch,"Daisuke Tajima, Jun Nishida, Pedro Lopes, Shunichi Kasahara",ACM TOCHI,2022,https://dl.acm.org/doi/10.1145/3489608,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719467337796-OY9QFA6S8UCGAQ67K55L/WT.jpeg?format=500w,,"Force-feedback enhances digital touch by enabling users to share non-verbal aspects such as rhythm, poses, and so on. To achieve this, interfaces actuate the user’s to touch involuntarily (using exoskeletons or electrical-muscle-stimulation); we refer to this as computer-driven touch. Unfortunately, forcing users to touch causes a loss of their sense of agency. While researchers found that delaying the timing of computer-driven touch preserves agency, they only considered the naïve case when user-driven touch is aligned with computer-driven touch. We argue this is unlikely as it assumes we can perfectly predict user-touches. But, what about all the remainder situations: when the haptics forces the user into an outcome they did not intend or assists the user in an outcome they would not achieve alone? We unveil, via an experiment, what happens in these novel situations. From our findings, we synthesize a framework that enables researchers of digital-touch systems to trade-off between haptic-assistance vs. sense-of-agency.",
Electrical Head Actuation,Electrical Head Actuation: Allowing Interactive Systems to Directly Manipulate Head Orientation,"Yudai Tanaka, Jun Nishida, Pedro Lopes",ACM CHI 2022,2022,https://dl.acm.org/doi/10.1145/3491102.3501910,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/14cdcd9f-c7f2-401d-9c18-0cbc0c3ade5d/EHA.png?format=500w,https://www.youtube.com/watch?v=vqpH9gNGpts,"We propose a novel interface concept in which interactive systems directly manipulate the user's head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. As the first exploration of EMS for head actuation, we characterized which muscles can be robustly actuated. Second, we evaluated the accuracy of our system for actuating participants' head orientation towards static targets and trajectories. Third, we demonstrated how it enables interactions not possible before by building a range of applications, such as (1) synchronizing head orientations of two users, which enables a user to communicate head nods to another user while listening to music, and (2) directly changing the user's head orientation to locate objects in AR. Finally, in our second study, participants felt that our head actuation contributed positively to their experience in four distinct applications.",People’s Choice Best Demo Award
Agency Drives Learning,Agency Drives Learning: Preserving Agency During Muscle Stimulation Speeds up Reaction Time,"Shunichi Kasahara, Kazuma Takada, Jun Nishida, Kazuhisa Shibata, Shinsuke Shimojo, Pedro Lopes",ACM CHI 2021,2021,https://dl.acm.org/doi/10.1145/3411764.3445147,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/b5a51b05-fc9c-4f9c-bb9a-a38084469f14/EMSfaster.jpeg?format=500w,https://www.youtube.com/watch?v=jVpUu9tCLtY,"Force feedback devices, such as motor-based exoskeletons or wearables based on electrical muscle stimulation (EMS), have the unique potential to accelerate users’ own reaction time (RT). However, this speedup has only been explored while the device is attached to the user. In fact, very little is known regarding whether this faster reaction time still occurs after the user removes the device from their bodies–this is precisely what we investigated by means of a simple reaction time (RT) experiment, in which participants were asked to tap as soon as they saw an LED flashing. Participants experienced this in three EMS conditions: (1) fast-EMS, the electrical impulses were synced with the LED; (2) agency-EMS, the electrical impulse was delivered 40ms faster than the participant’s own RT, which prior work has shown to preserve one’s sense of agency over this movement; and, (3) late-EMS: the impulse was delivered after the participant’s own RT. Our results revealed that the participants’ RT was significantly reduced by approximately 8ms (up to 20ms) only after training with the agency-EMS condition. This finding suggests that the prioritizing agency during EMS training is key to motor-adaptation, i.e., it enables a faster motor response even after the user has removed the EMS device from their body.",
Stereo-Smell,Stereo-Smell via Electrical Trigeminal Stimulation,"Jas Brooks, Shan-Yuan Teng, Jingxuan Wen, Romain Nith, Jun Nishida, Pedro Lopes",ACM CHI 2021,2021,https://dl.acm.org/doi/10.1145/3411764.3445300,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/eec97f0f-909e-4dc2-b0f8-f346a0e22916/ETS.jpg?format=500w,https://www.youtube.com/watch?v=sbmYBUSuM48,"We propose a novel type of olfactory device that creates a stereo-smell experience, i.e., directional information about the location of an odor, by rendering the readings of external odor sensors as trigeminal sensations using electrical stimulation of the user's nasal septum. The key is that the sensations from the trigeminal nerve, which arise from nerve-endings in the nose, are perceptually fused with those of the olfactory bulb (the brain region that senses smells). As such, we propose that electrically stimulating the trigeminal nerve is an ideal candidate for stereo-smell augmentation/substitution that, unlike other approaches, does not require implanted electrodes in the olfactory bulb. To realize this, we engineered a self-contained device that users wear across their nasal septum. Our device outputs by stimulating the user's trigeminal nerve using electrical impulses with variable pulse-widths; and it inputs by sensing the user's inhalations using a photoreflector. It measures 10x23 mm and communicates with external gas sensors using Bluetooth. In our user study, we found the key electrical waveform parameters that enable users to feel an odor's intensity (absolute electric charge) and direction (phase order and net charge). In our second study, we demonstrated that participants were able to localize a virtual smell source in the room by using our prototype without any previous training. Using these insights, our device enables expressive trigeminal sensations and could function as an assistive device for people with anosmia, who are unable to smell.",
HandMorph,HandMorph: a Passive Exoskeleton that Miniaturizes Grasp,"Jun Nishida, Soichiro Matsuda, Hiroshi Matsui, Shan-Yuan Teng, Ziwei Liu, Kenji Suzuki and Pedro Lopes",ACM UIST 2020,2020,https://dl.acm.org/doi/10.1145/3379337.3415875,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719299776548-0V95FIOEW6XJB53TZIZI/HM.jpg?format=500w,https://youtu.be/5o2wPy5hl0w,"We engineered an exoskeleton, which we call HandMorph, that approximates the experience of having a smaller grasping range. It uses mechanical links to transmit motion from the wearer's fingers to a smaller hand with five anatomically correct fingers. The result is that HandMorph miniaturizes a wearer's grasping range while transmitting haptic feedback. Unlike other size-illusions based on virtual reality, HandMorph achieves this in the user's real environment, preserving the user's physical and social contexts. As such, our device can be integrated into the user's workflow, e.g., to allow product designers to momentarily change their grasping range into that of a child while evaluating a toy prototype. In our first user study, we found that participants perceived objects as larger when wearing HandMorph, which suggests that their size perception was successfully transformed. In our second user study, we assessed the experience of using HandMorph in designing a simple toy trumpet for children. We found that participants felt more confident in their toy design when using HandMorph to validate its ergonomics.",Best Paper Award
CHILDHOOD,Egocentric Smaller-person Experience by Changing a Visual Perspective,"Jun Nishida, Soichiro Matsuda, Mika Oki, Hikaru Takatori, Kosuke Sato and Kenji Suzuki",ACM CHI 2019,2019,https://dl.acm.org/doi/10.1145/3290605.3300926,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719358711021-QOQRQ0I6ALXDU44H7ZH6/CH.jpg?format=500w,https://vimeo.com/120369920,"This paper explores how human perceptions, actions, and interactions can be changed through an embodied and active experience of being a smaller person in a real-world environment, which we call an egocentric smaller person experience. We developed a wearable visual translator that provides the perspective of a smaller person by shifting the wearer's eyesight level down to their waist using a head-mounted display and a stereo camera module, while allowing for field of view control through head movements. In this study, we investigated how the developed device can modify the wearer's body representation and experiences based on a field study conducted at a nursing school and museums, and through lab studies. It was observed that the participants changed their perceptions, actions, and interactions because they are considered to have perceived themselves as being smaller. Using this device, designers and teachers can understand the perspectives of other people in an existing environment.",Best Paper Honorable Mention Award
Preemptive Action,Preemptive Action: Accelerating Human Reaction using Electrical Muscle Stimulation Without Compromising Agency,"Shunichi Kasahara, Jun Nishida, and Pedro Lopes",ACM CHI 2019,2019,https://dl.acm.org/doi/10.1145/3290605.3300873,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719361793210-TKJYYUKXRYGJG4AC08T8/PA.jpeg?format=500w,https://www.youtube.com/watch?v=1BT8REEJibM,"We enable preemptive force-feedback systems to speed up human reaction time without fully compromising the user's sense of agency. Typically these interfaces actuate by means of electrical muscle stimulation (EMS) or mechanical actuators; they preemptively move the user to perform a task, such as to improve movement performance (e.g., EMS-assisted drumming). Unfortunately, when using preemptive force-feedback users do not feel in control and loose their sense of agency. We address this by actuating the user's body, using EMS, within a particular time window (160 ms after visual stimulus), which we found to speed up reaction time by 80 ms in our first study. With this preemptive timing, when the user and system move congruently, the user feels that they initiated the motion, yet their reaction time is faster than usual. As our second study demonstrated, this particular timing significantly increased agency when compared to the current practice in EMS-based devices. We conclude by illustrating, using examples from the HCI literature, how to leverage our findings to provide more agency to automated haptic interfaces.",
HC Integration,Next Steps for Human-Computer Integration: Challenges and Opportunities,"Florian ‘Floyd’ Mueller, Pedro Lopes, Paul Strohmeier, Wendy Ju, Caitlyn Seim, Martin Weigel, Suranga Nanayakkara, Marianna Obrist, Zhuying Li, Joseph Delfa, Jun Nishida, Elizabeth M. Gerber, Dag Svanaes, Jonathan Grudin, Stefan Greuter, Kai Kunze, Thomas Erickson, Steven Greenspan, Masahiko Inami, Joe Marshall, Harald Reiterer, Katrin Wolf, Jochen Meyer, Thecla Schiphorst, Dakuo Wang, Pattie Maes.",ACM CHI 2019,2019,https://dl.acm.org/doi/10.1145/3313831.3376242,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/261e0fa9-3ab5-4992-bc88-04b23503229a/Hint.png?format=500w,,"Human-Computer Integration (HInt) is an emerging paradigm in which computational and human systems are closely interwoven. Integrating computers with the human body is not new. however, we believe that with rapid technological advancements, increasing real-world deployments, and growing ethical and societal implications, it is critical to identify an agenda for future research. We present a set of challenges for HInt research, formulated over the course of a five-day workshop consisting of 29 experts who have designed, deployed and studied HInt systems. This agenda aims to guide researchers in a structured way towards a more coordinated and conscientious future of human-computer integration.",
bioSync,bioSync: A Paired Wearable Device for Blending Kinesthetic Experience,Jun Nishida and Kenji Suzuki,ACM CHI 2017,2017,https://dl.acm.org/doi/10.1145/3025453.3025829,https://images.squarespace-cdn.com/content/v1/66214dcc26841d4d27c5ba9b/1719300797192-IDBZHN8LBRDDRRLR50GG/bs_banner_2.jpeg?format=500w,https://vimeo.com/155394019,"We present a novel, paired, wearable system for combining the kinesthetic experiences of two persons. These devices allow users to sense and combine muscle contraction and joint rigidity bi-directionally. This is achieved through kinesthetic channels based on electromyogram (EMG) measurement and electrical muscle stimulation (EMS). We developed a pair of wearable kinesthetic input-output (I/O) devices called bioSync that uses specially designed electrodes to perform biosignal measurement and stimulation simultaneously on the same electrodes. In a user study, participants successfully evaluated the strength of their partners' muscle contractions while exerting their own muscles. We confirmed that the pair of devices could help participants synchronize their hand movements through tapping, without visual and auditory feedback. The proposed interpersonal kinesthetic communication system can be used to enhance interactions such as clinical gait rehabilitation and sports training, and facilitate sharing of physical experiences with Parkinson's patients, thereby enhancing understanding of the physical challenges they face in daily life.",